{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models, Layers, and Loss Functions with TensorFlow \n",
    "### *Week: 4*\n",
    "\n",
    "#### DeepLearningAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Compare Functional and Sequential APIs, discover new models you can build with the Functional API, and build a model that produces multiple outputs including a Siamese network.\n",
    "* Build custom loss functions (including the contrastive loss function used in a Siamese network) in order to measure how well a model is doing and help your neural network learn from training data. \n",
    "* Build off of existing standard layers to create custom layers for your models, customize a network layer with a lambda layer, understand the differences between them, learn what makes up a custom layer, and explore activation functions. \n",
    "* Build off of existing models to add custom functionality, learn how to define your own custom class instead of using the Functional or Sequential APIs, build models that can be inherited from the TensorFlow Model class, and build a residual network (ResNet) through defining a custom model class. \n",
    "\n",
    "\n",
    "The DeepLearning.AI TensorFlow: Advanced Techniques Specialization introduces the features of TensorFlow that provide learners with more control over their model architecture and tools that help them create and train advanced ML models.  \n",
    "\n",
    "This Specialization is for early and mid-career software and machine learning engineers with a foundational understanding of TensorFlow who are looking to expand their knowledge and skill set by learning advanced TensorFlow features to build powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Architectures with the Function API\n",
    "\n",
    "![Designed-network](newtork.png)\n",
    "\n",
    "```Python\n",
    "input_a = Input(shape=[1], name=\"Wide_Input\")\n",
    "input_b = Input(shape=[1], name=\"Deep_Input\")\n",
    "hidden_1 = Dense(30, activation=\"relu\")(input_b)\n",
    "hidden_2 = Dense(30, activation=\"relu\")(hidden_1)\n",
    "concat = concatenate([input_a, hidden_2])\n",
    "output = Dense(1, name=\"Output\",)(concat) # You can also add second out layer.\n",
    "\n",
    "model = Model(inputs=[input_a, input_b], outputs=[output]) # If you added second out layer, outputs list will be also changed.\n",
    "```\n",
    "\n",
    "![other-model](other_model.png)\n",
    "**This image different with above code, (second output layer)**\n",
    "\n",
    "```Python\n",
    "class WideAndDeepModel(Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = Dense(units, activation=activation)\n",
    "        self.hidden2 = Dense(units, activation=activation)\n",
    "        self.main_output = Dense(1)\n",
    "        self.aux_output = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "# The basic usage is\n",
    "\n",
    "model = WideAndDeepModel()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The first lab notebook](C1_W4_Lab_1_basic-model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model Class to simplify Complex Architecture\n",
    "\n",
    "#### The Model Class\n",
    "\n",
    "* Built-in training, evaluation, and prediction loops\n",
    "    \n",
    "`model.fit()`, `model.evaluate()`, `model.predict()`\n",
    "    \n",
    "* Saving and serialization APIs\n",
    "\n",
    "`model.save()`, `model.save_weights()`\n",
    "    \n",
    "* Summarization and visualization APIs\n",
    "\n",
    "`model.summary()`, `tf.keras.utils.plot_model`\n",
    "\n",
    "#### Limitations of Sequential/Functional APIs\n",
    "\n",
    "* Only suited to models that are Directed Acyclic Graphs of layers\n",
    "\n",
    ">In these cases, the data flows from the inputs to the outputs, and sometimes it's in multiple branches as we've seen already, but the direction is always the same, it never loops back during training or inference. \n",
    "\n",
    "`MobilNet`, `Inception`, `etc.`\n",
    "\n",
    "* More exotic architecture\n",
    "\n",
    ">Thus networks where recursion is used or dynamic networks where the architecture can change on the fly can be very difficult to build if you use these APIs. With sequential, they're impossible, with a functional APIs, they might be possible, but will involve a lot of coding. But with modal subclassing, it can become a bit easier to achieve these complex scenarios.\n",
    "\n",
    "`dynamic and recursive networks`\n",
    "\n",
    "#### Benefits of subclassing models\n",
    "\n",
    "* Extends how you've been building models\n",
    "\n",
    "* Continue to use functional and sequential code\n",
    "\n",
    "* Modular Architecture\n",
    "\n",
    "* Try out experiments quickly\n",
    "\n",
    "* Control flow in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Residual Networks\n",
    "\n",
    "![resnet_1](resnet1.png)\n",
    "\n",
    "![resnet_2](resnet2.png)\n",
    "\n",
    "![resnet_3](resnet3.png)\n",
    "\n",
    "```Python\n",
    "class CNNResidual(Layer):\n",
    "    def __init__(self, layers, filters, **kwargs):\n",
    "        super()__init__(**kwargs)\n",
    "        self.hidden = [Conv2D(filters, (3, 3), activation='relu') for _ in range(layers)]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden:\n",
    "            x = layer(x)\n",
    "        return inputs + x\n",
    "```\n",
    "\n",
    "```Python\n",
    "\n",
    "class DNNResidual(Layer):\n",
    "    def __init__(selfi layers, neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [Dense(neurons, activation=\"relu\") for _ in range(layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden:\n",
    "            x = layer(x)\n",
    "        return inputs + x\n",
    "```\n",
    "\n",
    "![resnet_4](resnet4.png)\n",
    "\n",
    "\n",
    "```Python\n",
    "class MyResidual(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.hidden1 = Dense(30, activation='relu')\n",
    "        self.block1 = CNNResidual(2, 32)\n",
    "        self.block2 = DNNResidual(2, 64)\n",
    "        self.out = Dense(1)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.hidden1(inputs)\n",
    "        x = self.block1(x)\n",
    "        for _ in range(1, 4):\n",
    "            x = self.block2(x)\n",
    "            \n",
    "        return self.out(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ResNet \n",
    "\n",
    "### Coding a Residual network with the Model class\n",
    "\n",
    "\n",
    "**ResNet**\n",
    "\n",
    "![Big-ResNet](resnet4-1.png)\n",
    "\n",
    "![Identity-Block](resnet4-2.png)\n",
    "\n",
    "![Identity ResNet Block with 1x1 Conv](resnet4-3.png)\n",
    "\n",
    "![Mini-ResNet](resnet4-4.png)\n",
    "\n",
    "\n",
    "#### We will be coding mini-ResNet model\n",
    "\n",
    "```Python\n",
    "class IdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(IdentityBlock, self).__init__(name='')\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.act = tf.keras.layers.Activation('relu')\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.add([x, input_tensor])\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "\n",
    "```Python\n",
    "class MiniResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MiniResNet, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.act = tf.keras.layers.Activation('relu')\n",
    "        self.max_pool = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.id1a = IdentityBlock(64, 3)\n",
    "        self.id1b = IdentityBlock(64, 3)\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.id1a(x)\n",
    "        x = self.id1b(x)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "# USAGE OF THE MODEL\n",
    "\n",
    "resnet = MiniResNet(10)\n",
    "resnet.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\n",
    "dataset = dataset.map(preprocess).batch(32)\n",
    "\n",
    "resnet.fit(dataset, epochs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The second lab notebook](C1_W4_Lab_2_resnet-example.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

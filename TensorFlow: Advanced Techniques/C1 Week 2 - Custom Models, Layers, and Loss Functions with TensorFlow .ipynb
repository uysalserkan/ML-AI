{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models, Layers, and Loss Functions with TensorFlow \n",
    "### *Week: 2*\n",
    "\n",
    "#### DeepLearningAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Compare Functional and Sequential APIs, discover new models you can build with the Functional API, and build a model that produces multiple outputs including a Siamese network.\n",
    "* Build custom loss functions (including the contrastive loss function used in a Siamese network) in order to measure how well a model is doing and help your neural network learn from training data. \n",
    "* Build off of existing standard layers to create custom layers for your models, customize a network layer with a lambda layer, understand the differences between them, learn what makes up a custom layer, and explore activation functions. \n",
    "* Build off of existing models to add custom functionality, learn how to define your own custom class instead of using the Functional or Sequential APIs, build models that can be inherited from the TensorFlow Model class, and build a residual network (ResNet) through defining a custom model class. \n",
    "\n",
    "\n",
    "The DeepLearning.AI TensorFlow: Advanced Techniques Specialization introduces the features of TensorFlow that provide learners with more control over their model architecture and tools that help them create and train advanced ML models.  \n",
    "\n",
    "This Specialization is for early and mid-career software and machine learning engineers with a foundational understanding of TensorFlow who are looking to expand their knowledge and skill set by learning advanced TensorFlow features to build powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions\n",
    "\n",
    "### Using Loss Functions\n",
    "\n",
    "```Python\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# OR\n",
    "\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "model.compile(loss=mean_squared_error(param=value), optimizer='sgd')\n",
    "```\n",
    "\n",
    "* We could use parameters in loss functions if loss functions is a object.\n",
    "\n",
    "### Creating a custom loss function\n",
    "\n",
    "```Python\n",
    "def my_loss_function(y_true, y_pred):\n",
    "    ...\n",
    "    return losses\n",
    "```\n",
    "\n",
    "* *y_true* will contain the labels. -The source of truth.\n",
    "* *y_pred* is predicted / calculated value in network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HuberLossFunction](huber-loss.png)\n",
    "\n",
    "##### [Reference](https://en.wikipedia.org/wiki/Huber_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Huber Loss Function\n",
    "\n",
    "```Python\n",
    "def my_huber_loss(y_true, y_pred):\n",
    "    threshold = 1\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= threshold\n",
    "    small_error_loss = tf.square(error) / 2\n",
    "    big_error_loss = threshold * (tf.abs(error) - (0.5 * threshold))\n",
    "    return tf.where(is_small_error, smal_error_loss, big_error_loss)\n",
    "```\n",
    "\n",
    "*Example*\n",
    "\n",
    "```Python\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='my_huber_loss')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The first lab notebook](C1_W2_Lab_1_huber-loss.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Hyperparameters and Classes\n",
    "\n",
    "### Adding Hyperparameters to Custom Loss Functions\n",
    "\n",
    "The *threshold* variable will be the parameter which will come outside thanks to the **wrapper function**.\n",
    "\n",
    "```Python\n",
    "def my_huber_loss_with_threshold(threshold):\n",
    "    def my_huber_loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) <= threshold\n",
    "        small_error_loss = tf.square(error) / 2\n",
    "        big_error_loss = threshold * (tf.abs(error) - (0.5 * threshold))\n",
    "        return tf.where(is_small_error, small_error_loss, big_error_loss)\n",
    "    return my_huber_los\n",
    "\n",
    "\n",
    "model.compile(optimizer='sgd', loss=my_huber_loss_with_threshold(threshold=1)) # Sending parameter\n",
    "```\n",
    "\n",
    "### Turning loss functions into Classes\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class MyHuberLoss(Loss):\n",
    "    threshold = 1\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) <= self.threshold\n",
    "        small_error_loss = tf.square(error) / 2\n",
    "        big_error_loss = self.threshold * (tf.abs(error) - (0.5 * self.threshold))\n",
    "        return tf.where(is_small_error, small_error_loss, big_error_loss)\n",
    "    \n",
    "## End    \n",
    "\n",
    "model.compile(optimizer='sgd', loss=MyHuberLoss(threshold=1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The second lab notebook](C1_W2_Lab_2_huber-object-loss.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss\n",
    "\n",
    "* If images are *similar*, produce feature vectors that are **very similar**.\n",
    "* If images are *different*, produce feature vectors that are **dissimilar**.\n",
    "* Based on the paper\n",
    "\n",
    ">\"Dimensionality Reduction by Learning an Invariant Mapping\"\n",
    ">-by R. Hadsell; S. Chopra; Y. LeCun\n",
    "\n",
    "##### [Reference](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "\n",
    "\n",
    "**Contrastive Loss - Formula** \n",
    "\\\\[Y * D^2 + (1 - Y) * max(margin - D, O)^2\\\\]\n",
    "\n",
    "\\\\[Y_{true} * Y_{pred}^2 + (1 - Y_{true}) * max(margin - Y_{pred}, O)^2\\\\]\n",
    "\n",
    "### Coding Contrastive Loss\n",
    "\n",
    "```Python\n",
    "# Custom Loss Function\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    magin=1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "# Usage of Custom Loss\n",
    "\n",
    "model.compile(loss=contrastive_loss, optimizer=RMSprop())\n",
    "\n",
    "\n",
    "# Custom Loss Function with Arguments\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "    return contrastive_loss\n",
    "\n",
    "\n",
    "# Usage of Wrapper Loss Function\n",
    "\n",
    "model.compile(loss=contrastive_loss_with_margin(margin=0.6), optimizer=rms)\n",
    "```\n",
    "\n",
    "#### OOP\n",
    "\n",
    "```Python\n",
    "class ContrastiveLoss(Loss):\n",
    "    margin = 0\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(self.margin - y_pred, 0))\n",
    "        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "    \n",
    "# Usage of OOP Loss\n",
    "model.compile(loss=ContrastiveLoss(margin=1), optimizer=rms)\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models, Layers, and Loss Functions with TensorFlow \n",
    "### *Week: 3*\n",
    "\n",
    "#### DeepLearningAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Compare Functional and Sequential APIs, discover new models you can build with the Functional API, and build a model that produces multiple outputs including a Siamese network.\n",
    "* Build custom loss functions (including the contrastive loss function used in a Siamese network) in order to measure how well a model is doing and help your neural network learn from training data. \n",
    "* Build off of existing standard layers to create custom layers for your models, customize a network layer with a lambda layer, understand the differences between them, learn what makes up a custom layer, and explore activation functions. \n",
    "* Build off of existing models to add custom functionality, learn how to define your own custom class instead of using the Functional or Sequential APIs, build models that can be inherited from the TensorFlow Model class, and build a residual network (ResNet) through defining a custom model class. \n",
    "\n",
    "\n",
    "The DeepLearning.AI TensorFlow: Advanced Techniques Specialization introduces the features of TensorFlow that provide learners with more control over their model architecture and tools that help them create and train advanced ML models.  \n",
    "\n",
    "This Specialization is for early and mid-career software and machine learning engineers with a foundational understanding of TensorFlow who are looking to expand their knowledge and skill set by learning advanced TensorFlow features to build powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Lambda Layer\n",
    "\n",
    "### Introduction to Lambda Layers\n",
    "\n",
    ">**Lambda Layer**:\n",
    ">This is a layer type that can be used to execute arbitrary code. The purpose of the lambda layer, like I said, is to execute an arbitrary function within a sequential or a functional API model. It's best-suited for something quick and simple or if you want to experiment.\n",
    "\n",
    "The simplest Lambda Layer\n",
    "\n",
    "```Python\n",
    "tf.keras.layers.Lambda(lambda x: tf.abs(x))\n",
    "```\n",
    "\n",
    "#### We could change 'relu' layer to lambda layer in below code\n",
    "\n",
    "```Python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Same loss and accuracy values with this\n",
    "\n",
    "mode = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape(28, 28)),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.abs(x)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "### Custom Functions from Lambda Layers\n",
    "\n",
    ">Previously you saw how to use Lambda layers to execute arbitrary code within your layer definition. Another example is to have a custom function that the Lambda layer can call in order to encapsulate your code. So for example, if you wanted to implement a modified Relu with a threshold, you could do so. \n",
    "\n",
    "\n",
    "```Python\n",
    "def my_relu(x):\n",
    "    return tf.max(0.0, x) # You can tweak yout relu function.\n",
    "\n",
    "model = tf.keras.layers.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Lambda(my_relu)\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The first lab notebook](C1_W3_Lab_1_lambda-layer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Custom Layers\n",
    "\n",
    "### Architecture of a Custom Layer\n",
    "\n",
    "**Some commonly used layers**\n",
    "\n",
    "|Convolutional|Recurrent|Pooling|Merge|Activations|Core|\n",
    "|--|--|--|--|--|--|\n",
    "|Conv1D, Conv2D, Conv3D|LSTM|MaxPooling2D|Add|LeakyReLU|Activation|\n",
    "|SeperableConv2D|GRU|AveragePooling2D|Subtract|PReLU|Lambda|\n",
    "|DepthwiseConv2D| |GlobalAveragePooling2D|Multiply|ELU|Input|\n",
    "||||||Dense|\n",
    "||||||Dropout|\n",
    "||||||BatchNormalization|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Layer?\n",
    "![A-layer](what_is_layer.png)\n",
    "\n",
    ">Typically it's a class that collects parameters that encapsulates state and computation to achieve the layers purpose in a neural network. Whether you're using it in the sequential or functional API, every model architecture design item is a layer. \n",
    "\n",
    ">When we say states, consider this to be a variable, something that makes a particular instance of a layer unique. These variables can be trainable where during model.fit, TensorFlow can tweak their values to test for better performance or they can be non-trainable, where they might be used for some other feature. \n",
    "\n",
    ">Computation is the means of transforming a batch of inputs into a batch of outputs. It's typically called the forward pass in neural networks, where a calculation is made and then pass to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "class SimpleDense(Layer):\n",
    "    \n",
    "    def __init__(self, units=25):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(selfi input_shape): # Create the state of the layer (weights)\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            name=\"kernel\",\n",
    "            initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n",
    "            trainable=True\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            name=\"bias\",\n",
    "            initial_value = b_init(shape=(self.units,), dtype='float32'),\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs): # Defines the computation from inputs to outputs\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    \n",
    "# In action\n",
    "\n",
    "my_dense = SimpleDense(units=1)\n",
    "x = tf.ones((1, 1))\n",
    "y = my_dense(x)\n",
    "print(my_dense.variables)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network with your Custom Layer\n",
    "\n",
    "The hello world nn.\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model = tf.keras.Sequential([SimpleDense(units=1)])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "print(model.predict([10.0]))\n",
    "```\n",
    "\n",
    "\n",
    "**Change fashion mnist code with our newest code**\n",
    "\n",
    "##### OLD\n",
    "\n",
    "```Python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "```\n",
    "##### NEW\n",
    "\n",
    "```Python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape(28, 28)),\n",
    "    SimpleDense(128),\n",
    "    tf.keras.layers.Lambda(my_relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The second lab notebook](C1_W3_Lab_2_custom-dense-layer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Custom Layers\n",
    "\n",
    ">ou saw how these can be the basic building blocks of a deep neural network by being able to add them in layers, and you also saw how to build some simple machine learning models using them. But your custom layer was missing the ability to do an activation on them. There was a workaround that we did using a lambda layer, but it's simpler and cleaner to specify an activation function on a layer. Let's take a look at how to expand our dense class to be able to do that. \n",
    "\n",
    "#### Change Basic Mnist Model\n",
    "\n",
    "```Python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape(28, 28)),\n",
    "    SimpleDense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "```\n",
    "\n",
    "*Customize our **Simple Dense* layer for activation*\n",
    "\n",
    "```Python\n",
    "class SimpleDense(Layer):\n",
    "    \n",
    "    def __init__(self, units=25, activation=None):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [The third lab notebook](C1_W3_Lab_3_custom-layer-activation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

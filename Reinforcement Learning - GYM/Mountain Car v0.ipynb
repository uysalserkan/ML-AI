{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colored-amplifier",
   "metadata": {},
   "source": [
    "# Open.ai GYM Mountain Car v0 \n",
    "\n",
    "[hidden_link](https://blog.tanka.la/2018/10/19/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medium-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras as K\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sonic-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "goal_steps = 200\n",
    "score_requirement = -198\n",
    "initial_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_random_game_first():\n",
    "    for step_index in range(goal_steps):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Step {}:\".format(step_index))\n",
    "        print(\"action: {}\".format(action))\n",
    "        print(\"observation: {}\".format(observation))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"done: {}\".format(done))\n",
    "        print(\"info: {}\".format(info))\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affiliated-trout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 0\n",
      "observation: [-0.4824299  -0.00131779]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 2\n",
      "observation: [-0.48305567 -0.00062577]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 0\n",
      "observation: [-0.48498477 -0.00192909]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 1\n",
      "observation: [-0.48720282 -0.00221805]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 0\n",
      "observation: [-0.4906933  -0.00349048]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 1\n",
      "observation: [-0.49443017 -0.00373687]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [-0.49938552 -0.00495536]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 2\n",
      "observation: [-0.50352232 -0.0041368 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 1\n",
      "observation: [-0.5078096  -0.00428728]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 0\n",
      "observation: [-0.51321525 -0.00540565]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 2\n",
      "observation: [-0.51769876 -0.00448352]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 0\n",
      "observation: [-0.52322653 -0.00552776]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 0\n",
      "observation: [-0.52975708 -0.00653056]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 1\n",
      "observation: [-0.53624145 -0.00648437]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 2\n",
      "observation: [-0.54163103 -0.00538957]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 1\n",
      "observation: [-0.54688543 -0.0052544 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 2\n",
      "observation: [-0.55096532 -0.00407989]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 2\n",
      "observation: [-0.55384019 -0.00287487]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 2\n",
      "observation: [-0.55548856 -0.00164837]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 0\n",
      "observation: [-0.55789812 -0.00240956]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 1\n",
      "observation: [-0.56005089 -0.00215277]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 1\n",
      "observation: [-0.56193082 -0.00187993]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 1\n",
      "observation: [-0.56352389 -0.00159307]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 1\n",
      "observation: [-0.56481823 -0.00129435]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 1\n",
      "observation: [-0.56580422 -0.00098599]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 0\n",
      "observation: [-0.56747451 -0.00167029]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 0\n",
      "observation: [-0.56981668 -0.00234217]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 2\n",
      "observation: [-0.57081333 -0.00099665]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 1\n",
      "observation: [-0.57145705 -0.00064372]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 0\n",
      "observation: [-0.57274307 -0.00128602]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 1\n",
      "observation: [-0.57366184 -0.00091877]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 0\n",
      "observation: [-0.57520654 -0.0015447 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 1\n",
      "observation: [-0.57636573 -0.00115919]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 2\n",
      "observation: [-5.76130822e-01  2.34910234e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 0\n",
      "observation: [-5.7650355e-01 -3.7272829e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 0\n",
      "observation: [-0.57748116 -0.00097761]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 2\n",
      "observation: [-5.77056402e-01  4.24753993e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 2\n",
      "observation: [-0.57523243  0.00182397]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 2\n",
      "observation: [-0.57202276  0.00320968]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 2\n",
      "observation: [-0.56745118  0.00457158]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 2\n",
      "observation: [-0.56155165  0.00589952]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 1\n",
      "observation: [-0.5553681   0.00618356]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 2\n",
      "observation: [-0.54794663  0.00742147]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 2\n",
      "observation: [-0.53934272  0.00860391]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 0\n",
      "observation: [-0.53162077  0.00772195]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 0\n",
      "observation: [-0.52483866  0.00678211]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 1\n",
      "observation: [-0.51804726  0.00679141]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 1\n",
      "observation: [-0.51129749  0.00674977]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 1\n",
      "observation: [-0.50463995  0.00665753]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 2\n",
      "observation: [-0.49712453  0.00751542]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 1\n",
      "observation: [-0.48980746  0.00731707]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 0\n",
      "observation: [-0.4837434   0.00606407]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 2\n",
      "observation: [-0.47697753  0.00676587]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 2\n",
      "observation: [-0.46956018  0.00741735]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 2\n",
      "observation: [-0.46154636  0.00801383]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 1\n",
      "observation: [-0.45399524  0.00755112]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 2\n",
      "observation: [-0.44596236  0.00803288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 1\n",
      "observation: [-0.4385065   0.00745585]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 0\n",
      "observation: [-0.43268193  0.00582457]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 2\n",
      "observation: [-0.42653081  0.00615112]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 2\n",
      "observation: [-0.42009746  0.00643335]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 0\n",
      "observation: [-0.41542795  0.00466951]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 1\n",
      "observation: [-0.41155556  0.00387239]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 1\n",
      "observation: [-0.40850776  0.0030478 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 0\n",
      "observation: [-0.40730609  0.00120167]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 2\n",
      "observation: [-0.40595903  0.00134706]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 1\n",
      "observation: [-0.40547607  0.00048296]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 0\n",
      "observation: [-0.4068606  -0.00138453]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 1\n",
      "observation: [-0.40910288 -0.00224228]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 0\n",
      "observation: [-0.4131871  -0.00408421]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 2\n",
      "observation: [-0.41708434 -0.00389724]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 0\n",
      "observation: [-0.42276691 -0.00568257]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 1\n",
      "observation: [-0.42919425 -0.00642733]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 2\n",
      "observation: [-0.43532019 -0.00612594]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 0\n",
      "observation: [-0.4431005  -0.00778032]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 2\n",
      "observation: [-0.4504787 -0.0073782]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 1\n",
      "observation: [-0.45840092 -0.00792222]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 0\n",
      "observation: [-0.46780901 -0.00940809]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 2\n",
      "observation: [-0.47663357 -0.00882456]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 1\n",
      "observation: [-0.4858092  -0.00917563]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 0\n",
      "observation: [-0.49626765 -0.01045845]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 1\n",
      "observation: [-0.50693085 -0.0106632 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 1\n",
      "observation: [-0.51771901 -0.01078816]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 0\n",
      "observation: [-0.52955127 -0.01183226]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 2\n",
      "observation: [-0.54033889 -0.01078762]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 0\n",
      "observation: [-0.552001   -0.01166212]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 0\n",
      "observation: [-0.56445036 -0.01244936]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 0\n",
      "observation: [-0.5775941  -0.01314374]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 1\n",
      "observation: [-0.59033464 -0.01274054]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 2\n",
      "observation: [-0.601578   -0.01124336]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 2\n",
      "observation: [-0.61124183 -0.00966384]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 0\n",
      "observation: [-0.62125589 -0.01001406]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 0\n",
      "observation: [-0.63154796 -0.01029206]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 1\n",
      "observation: [-0.64104448 -0.00949652]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 2\n",
      "observation: [-0.64867827 -0.00763379]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 95:\n",
      "action: 1\n",
      "observation: [-0.65539583 -0.00671756]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 1\n",
      "observation: [-0.66115046 -0.00575463]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n",
      "action: 2\n",
      "observation: [-0.66490249 -0.00375203]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 2\n",
      "observation: [-0.66662619 -0.0017237 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 1\n",
      "observation: [-0.66730981 -0.00068361]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 0\n",
      "observation: [-6.67948669e-01 -6.38862208e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 2\n",
      "observation: [-0.66653843  0.00141024]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 2\n",
      "observation: [-0.6630887   0.00344973]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 0\n",
      "observation: [-0.65962306  0.00346564]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 0\n",
      "observation: [-0.65616531  0.00345774]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 2\n",
      "observation: [-0.65073933  0.00542599]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n",
      "action: 0\n",
      "observation: [-0.64538274  0.00535659]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 2\n",
      "observation: [-0.63813295  0.00724978]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 0\n",
      "observation: [-0.63104096  0.00709199]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 0\n",
      "observation: [-0.62415704  0.00688392]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 1\n",
      "observation: [-0.61653032  0.00762672]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 1\n",
      "observation: [-0.60821561  0.00831471]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 1\n",
      "observation: [-0.59927306  0.00894255]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 2\n",
      "observation: [-0.58876782  0.01050524]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 1\n",
      "observation: [-0.57777691  0.0109909 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 0\n",
      "observation: [-0.56738146  0.01039545]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 0\n",
      "observation: [-0.55765859  0.00972288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 0\n",
      "observation: [-0.5486807   0.00897788]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 1\n",
      "observation: [-0.53951489  0.00916582]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 1\n",
      "observation: [-0.53022974  0.00928514]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 1\n",
      "observation: [-0.52089487  0.00933487]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 2\n",
      "observation: [-0.51058027  0.01031459]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 1\n",
      "observation: [-0.50036329  0.01021698]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 2\n",
      "observation: [-0.48932044  0.01104286]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 1\n",
      "observation: [-0.47853422  0.01078622]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 0\n",
      "observation: [-0.46908495  0.00944927]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 0\n",
      "observation: [-0.46104272  0.00804223]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 0\n",
      "observation: [-0.45446691  0.00657581]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 2\n",
      "observation: [-0.44740588  0.00706103]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 1\n",
      "observation: [-0.44091133  0.00649455]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 1\n",
      "observation: [-0.4350306   0.00588073]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 0\n",
      "observation: [-0.43080634  0.00422426]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 2\n",
      "observation: [-0.42626907  0.00453727]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 1\n",
      "observation: [-0.42245145  0.00381763]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 2\n",
      "observation: [-0.41838084  0.00407061]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 2\n",
      "observation: [-0.41408633  0.00429451]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 1\n",
      "observation: [-0.41059846  0.00348787]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 0\n",
      "observation: [-0.40894196  0.0016565 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 0\n",
      "observation: [-4.09128524e-01 -1.86565923e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 0\n",
      "observation: [-0.41115684 -0.00202832]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 1\n",
      "observation: [-0.41401257 -0.00285573]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 1\n",
      "observation: [-0.41767547 -0.0036629 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 0\n",
      "observation: [-0.42311949 -0.00544402]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 2\n",
      "observation: [-0.42830575 -0.00518626]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 1\n",
      "observation: [-0.43419701 -0.00589126]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 2\n",
      "observation: [-0.43975077 -0.00555376]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 1\n",
      "observation: [-0.44592678 -0.00617601]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 2\n",
      "observation: [-0.45168008 -0.00575329]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 2\n",
      "observation: [-0.45696859 -0.00528851]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 0\n",
      "observation: [-0.4637535  -0.00678491]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 2\n",
      "observation: [-0.46998485 -0.00623135]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 0\n",
      "observation: [-0.47761657 -0.00763172]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 1\n",
      "observation: [-0.48559206 -0.00797549]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 2\n",
      "observation: [-0.49285199 -0.00725993]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 1\n",
      "observation: [-0.50034219 -0.0074902 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 0\n",
      "observation: [-0.50900667 -0.00866448]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 0\n",
      "observation: [-0.51878056 -0.00977389]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 1\n",
      "observation: [-0.52859058 -0.00981002]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 2\n",
      "observation: [-0.53736317 -0.00877259]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 2\n",
      "observation: [-0.54503256 -0.00766938]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 2\n",
      "observation: [-0.5515413  -0.00650874]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 2\n",
      "observation: [-0.55684072 -0.00529942]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 1\n",
      "observation: [-0.56189123 -0.00505052]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 0\n",
      "observation: [-0.56765519 -0.00576395]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 0\n",
      "observation: [-0.57408968 -0.00643449]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 1\n",
      "observation: [-0.58014693 -0.00605726]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 1\n",
      "observation: [-0.58578211 -0.00563518]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 1\n",
      "observation: [-0.59095361 -0.0051715 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 1\n",
      "observation: [-0.59562338 -0.00466977]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 1\n",
      "observation: [-0.59975717 -0.00413378]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 2\n",
      "observation: [-0.60232471 -0.00256755]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 0\n",
      "observation: [-0.60530729 -0.00298258]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 1\n",
      "observation: [-0.60768318 -0.00237588]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 2\n",
      "observation: [-0.60843509 -0.00075192]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 2\n",
      "observation: [-0.60755758  0.00087751]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 1\n",
      "observation: [-0.60605702  0.00150056]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 2\n",
      "observation: [-0.60294431  0.00311271]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 0\n",
      "observation: [-0.60024211  0.0027022 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 1\n",
      "observation: [-0.59697014  0.00327197]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 1\n",
      "observation: [-0.59315231  0.00381782]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 2\n",
      "observation: [-0.58781662  0.0053357 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 1\n",
      "observation: [-0.58200226  0.00581436]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 0\n",
      "observation: [-0.57675211  0.00525015]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 1\n",
      "observation: [-0.571105    0.00564711]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 1\n",
      "observation: [-0.5651028  0.0060022]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 2\n",
      "observation: [-0.55779012  0.00731268]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 2\n",
      "observation: [-0.54922145  0.00856866]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 0\n",
      "observation: [-0.54146081  0.00776064]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 1\n",
      "observation: [-0.53356626  0.00789455]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 1\n",
      "observation: [-0.52559697  0.00796929]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 190:\n",
      "action: 1\n",
      "observation: [-0.51761269  0.00798428]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 2\n",
      "observation: [-0.50867331  0.00893938]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 2\n",
      "observation: [-0.49884583  0.00982748]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 1\n",
      "observation: [-0.48920382  0.009642  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 194:\n",
      "action: 0\n",
      "observation: [-0.48081933  0.0083845 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 2\n",
      "observation: [-0.47175479  0.00906453]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 0\n",
      "observation: [-0.46407752  0.00767727]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 0\n",
      "observation: [-0.45784429  0.00623323]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 0\n",
      "observation: [-0.45310103  0.00474326]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 1\n",
      "observation: [-0.44888257  0.00421846]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "play_a_random_game_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-western",
   "metadata": {},
   "source": [
    "## Creating data from action steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorrect-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_data_preparation():\n",
    "    training_data = []\n",
    "    scores = []\n",
    "    \n",
    "    for index in range(initial_games):\n",
    "        score = 0\n",
    "        memory = []\n",
    "        prev_observation = []\n",
    "        \n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0,3)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(prev_observation) > 0:\n",
    "                memory.append([prev_observation, action])\n",
    "                \n",
    "            prev_observation = observation\n",
    "            \n",
    "            if observation[0] > -0.2:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        if score >= score_requirement:\n",
    "            scores.append(score)\n",
    "\n",
    "            for data in memory:\n",
    "                if data[1] == 0:\n",
    "                    output = [1,0,0]\n",
    "                elif data[1] == 1:\n",
    "                    output = [0,1,0]\n",
    "                elif data[1] == 2:\n",
    "                    output = [0,0,1]\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        env.reset()\n",
    "        \n",
    "    env.close()\n",
    "    print(scores)\n",
    "    \n",
    "    return training_data\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olive-doctor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-180.0, -174.0, -184.0, -182.0, -198.0, -154.0, -178.0, -176.0, -190.0, -182.0, -184.0, -186.0, -194.0, -192.0, -182.0, -190.0, -186.0, -176.0, -184.0, -188.0, -176.0, -186.0, -168.0, -188.0, -186.0, -180.0, -174.0, -192.0, -178.0, -170.0, -194.0, -188.0, -184.0, -176.0, -180.0, -172.0]\n"
     ]
    }
   ],
   "source": [
    "train_data = action_data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "featured-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "    model = K.models.Sequential([\n",
    "        K.layers.Dense(128, input_dim=input_size, activation='relu'),\n",
    "        K.layers.Dense(52, activation='relu'),\n",
    "        K.layers.Dense(output_size, activation='linear'),\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=K.optimizers.Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "protective-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    Y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    \n",
    "    model = build_model(len(X[0]), len(Y[0]))\n",
    "    \n",
    "    model.fit(X, Y, epochs= 10)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wired-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "224/224 [==============================] - 1s 872us/step - loss: 0.2513\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 0s 840us/step - loss: 0.2230\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 0s 860us/step - loss: 0.2222\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 0s 849us/step - loss: 0.2214\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 0s 801us/step - loss: 0.2214\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 0s 845us/step - loss: 0.2209\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 0s 865us/step - loss: 0.2204\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 0s 855us/step - loss: 0.2204\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 0s 850us/step - loss: 0.2203\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 0s 857us/step - loss: 0.2202\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "encouraging-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    scores = []\n",
    "    choices = []\n",
    "    \n",
    "    for each in range(100):\n",
    "        score = 0\n",
    "        memory = []\n",
    "        prev_obs = []\n",
    "        \n",
    "        for step in range(goal_steps):\n",
    "            env.render()\n",
    "            \n",
    "            if len(prev_obs) == 0:\n",
    "                action = random.randrange(0, 2)\n",
    "            \n",
    "            else:\n",
    "                action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
    "                \n",
    "            choices.append(action)\n",
    "            \n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            prev_obs = new_obs\n",
    "            \n",
    "            memory.append([new_obs, action])\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    env.reset()\n",
    "    \n",
    "    \n",
    "    scores.append(score)\n",
    "    env.close()\n",
    "    print(scores)\n",
    "    print('Average Score:',sum(scores)/len(scores))\n",
    "    print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-canyon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0]\n",
      "Average Score: -1.0\n",
      "choice 1:0.26339285714285715  choice 0:0.39285714285714285 choice 2:0.34375\n"
     ]
    }
   ],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-visibility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
